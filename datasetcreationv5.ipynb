{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 10000  # Define a chunk size that fits into memory\n",
    "\n",
    "# Lists to store chunks\n",
    "X_segments_chunks = []\n",
    "y_labels_chunks = []\n",
    "X_rp_images_chunks = []\n",
    "X_gaf_images_chunks = []\n",
    "X_mtf_images_chunks = []\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        X_segments_chunks.append(hdf5_file['segments'][i:i+chunk_size])\n",
    "        y_labels_chunks.append(hdf5_file['labels'][i:i+chunk_size])\n",
    "        X_rp_images_chunks.append(hdf5_file['rp_images'][i:i+chunk_size])\n",
    "        X_gaf_images_chunks.append(hdf5_file['gaf_images'][i:i+chunk_size])\n",
    "        X_mtf_images_chunks.append(hdf5_file['mtf_images'][i:i+chunk_size])\n",
    "\n",
    "# Concatenate the chunks into a single array\n",
    "X_segments = np.concatenate(X_segments_chunks, axis=0)\n",
    "y_labels = np.concatenate(y_labels_chunks, axis=0)\n",
    "X_rp_images = np.concatenate(X_rp_images_chunks, axis=0)\n",
    "X_gaf_images = np.concatenate(X_gaf_images_chunks, axis=0)\n",
    "X_mtf_images = np.concatenate(X_mtf_images_chunks, axis=0)\n",
    "\n",
    "# Ensure we have a sufficient number of samples for each class\n",
    "class_counts = Counter(y_labels)\n",
    "total_counts = sum(class_counts.values())\n",
    "\n",
    "# Calculate the proportions of each class\n",
    "class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "# Set the number of samples for the smallest class (VEB in this case)\n",
    "min_samples_per_class = min(class_counts.values())\n",
    "\n",
    "# Create a balanced subset\n",
    "balanced_indices = []\n",
    "for class_label in class_counts:\n",
    "    class_indices = np.where(y_labels == class_label)[0]\n",
    "    balanced_indices.extend(class_indices)\n",
    "\n",
    "# Select the balanced subset\n",
    "X_rp_subset = X_rp_images[balanced_indices]\n",
    "X_gaf_subset = X_gaf_images[balanced_indices]\n",
    "X_mtf_subset = X_mtf_images[balanced_indices]\n",
    "y_subset = y_labels[balanced_indices]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_subset, X_gaf_subset, X_mtf_subset, y_subset, test_size=0.3, stratify=y_subset, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data\n",
    "save_dir = './split_datav5_full'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, 'X_rp_train.npy'), X_rp_train)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_train.npy'), X_gaf_train)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_train.npy'), X_mtf_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_val.npy'), X_rp_val)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_val.npy'), X_gaf_val)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_val.npy'), X_mtf_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_test.npy'), X_rp_test)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_test.npy'), X_gaf_test)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_test.npy'), X_mtf_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#using smaller chunks\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 5000  # Define a smaller chunk size to fit into memory\n",
    "\n",
    "# Lists to store chunks\n",
    "X_segments_chunks = []\n",
    "y_labels_chunks = []\n",
    "X_rp_images_chunks = []\n",
    "X_gaf_images_chunks = []\n",
    "X_mtf_images_chunks = []\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        X_segments_chunks.append(hdf5_file['segments'][i:i+chunk_size])\n",
    "        y_labels_chunks.append(hdf5_file['labels'][i:i+chunk_size])\n",
    "        X_rp_images_chunks.append(hdf5_file['rp_images'][i:i+chunk_size])\n",
    "        X_gaf_images_chunks.append(hdf5_file['gaf_images'][i:i+chunk_size])\n",
    "        X_mtf_images_chunks.append(hdf5_file['mtf_images'][i:i+chunk_size])\n",
    "\n",
    "# Concatenate the chunks into a single array\n",
    "X_segments = np.concatenate(X_segments_chunks, axis=0)\n",
    "y_labels = np.concatenate(y_labels_chunks, axis=0)\n",
    "X_rp_images = np.concatenate(X_rp_images_chunks, axis=0)\n",
    "X_gaf_images = np.concatenate(X_gaf_images_chunks, axis=0)\n",
    "X_mtf_images = np.concatenate(X_mtf_images_chunks, axis=0)\n",
    "\n",
    "# Ensure we have a sufficient number of samples for each class\n",
    "class_counts = Counter(y_labels)\n",
    "total_counts = sum(class_counts.values())\n",
    "\n",
    "# Calculate the proportions of each class\n",
    "class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "# Create a balanced subset\n",
    "balanced_indices = []\n",
    "for class_label in class_counts:\n",
    "    class_indices = np.where(y_labels == class_label)[0]\n",
    "    balanced_indices.extend(class_indices)\n",
    "\n",
    "# Select the balanced subset\n",
    "X_rp_subset = X_rp_images[balanced_indices]\n",
    "X_gaf_subset = X_gaf_images[balanced_indices]\n",
    "X_mtf_subset = X_mtf_images[balanced_indices]\n",
    "y_subset = y_labels[balanced_indices]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_subset, X_gaf_subset, X_mtf_subset, y_subset, test_size=0.3, stratify=y_subset, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data\n",
    "save_dir = './split_datav5_full'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, 'X_rp_train.npy'), X_rp_train)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_train.npy'), X_gaf_train)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_train.npy'), X_mtf_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_val.npy'), X_rp_val)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_val.npy'), X_gaf_val)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_val.npy'), X_mtf_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_test.npy'), X_rp_test)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_test.npy'), X_gaf_test)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_test.npy'), X_mtf_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#process each chunk sequentially\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(X_rp_images_chunk, X_gaf_images_chunk, X_mtf_images_chunk, y_labels_chunk):\n",
    "    # Ensure we have a sufficient number of samples for each class\n",
    "    class_counts = Counter(y_labels_chunk)\n",
    "    total_counts = sum(class_counts.values())\n",
    "\n",
    "    # Calculate the proportions of each class\n",
    "    class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "    # Create a balanced subset\n",
    "    balanced_indices = []\n",
    "    for class_label in class_counts:\n",
    "        class_indices = np.where(y_labels_chunk == class_label)[0]\n",
    "        balanced_indices.extend(class_indices)\n",
    "\n",
    "    # Select the balanced subset\n",
    "    X_rp_subset = X_rp_images_chunk[balanced_indices]\n",
    "    X_gaf_subset = X_gaf_images_chunk[balanced_indices]\n",
    "    X_mtf_subset = X_mtf_images_chunk[balanced_indices]\n",
    "    y_subset = y_labels_chunk[balanced_indices]\n",
    "\n",
    "    return X_rp_subset, X_gaf_subset, X_mtf_subset, y_subset\n",
    "\n",
    "# Lists to store the processed data\n",
    "X_rp_images_processed = []\n",
    "X_gaf_images_processed = []\n",
    "X_mtf_images_processed = []\n",
    "y_labels_processed = []\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 5000  # Define a smaller chunk size to fit into memory\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        X_rp_images_chunk = hdf5_file['rp_images'][i:i+chunk_size]\n",
    "        X_gaf_images_chunk = hdf5_file['gaf_images'][i:i+chunk_size]\n",
    "        X_mtf_images_chunk = hdf5_file['mtf_images'][i:i+chunk_size]\n",
    "        y_labels_chunk = hdf5_file['labels'][i:i+chunk_size]\n",
    "\n",
    "        # Process each chunk\n",
    "        X_rp_subset, X_gaf_subset, X_mtf_subset, y_subset = process_chunk(\n",
    "            X_rp_images_chunk, X_gaf_images_chunk, X_mtf_images_chunk, y_labels_chunk)\n",
    "        \n",
    "        X_rp_images_processed.append(X_rp_subset)\n",
    "        X_gaf_images_processed.append(X_gaf_subset)\n",
    "        X_mtf_images_processed.append(X_mtf_subset)\n",
    "        y_labels_processed.append(y_subset)\n",
    "\n",
    "# Concatenate the processed chunks into a single array\n",
    "X_rp_images = np.concatenate(X_rp_images_processed, axis=0)\n",
    "X_gaf_images = np.concatenate(X_gaf_images_processed, axis=0)\n",
    "X_mtf_images = np.concatenate(X_mtf_images_processed, axis=0)\n",
    "y_labels = np.concatenate(y_labels_processed, axis=0)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_images, X_gaf_images, X_mtf_images, y_labels, test_size=0.3, stratify=y_labels, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data\n",
    "save_dir = './split_datav5_full'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, 'X_rp_train.npy'), X_rp_train)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_train.npy'), X_gaf_train)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_train.npy'), X_mtf_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_val.npy'), X_rp_val)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_val.npy'), X_gaf_val)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_val.npy'), X_mtf_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_test.npy'), X_rp_test)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_test.npy'), X_gaf_test)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_test.npy'), X_mtf_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not enough free space to write 2440705212 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_mtf_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), X_mtf_train)\n\u001b[1;32m     81\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), y_train)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_rp_val.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_rp_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_gaf_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), X_gaf_val)\n\u001b[1;32m     85\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_mtf_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), X_mtf_val)\n",
      "File \u001b[0;32m~/miniconda3/envs/newtestv2/lib/python3.12/site-packages/numpy/lib/npyio.py:546\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/newtestv2/lib/python3.12/site-packages/numpy/lib/format.py:730\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m--> 730\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    733\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    734\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mOSError\u001b[0m: Not enough free space to write 2440705212 bytes"
     ]
    }
   ],
   "source": [
    "#Process and Save Chunks Independently:\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to process each chunk and save it to disk\n",
    "def process_and_save_chunk(X_rp_images_chunk, X_gaf_images_chunk, X_mtf_images_chunk, y_labels_chunk, chunk_id, save_dir):\n",
    "    # Ensure we have a sufficient number of samples for each class\n",
    "    class_counts = Counter(y_labels_chunk)\n",
    "    total_counts = sum(class_counts.values())\n",
    "\n",
    "    # Calculate the proportions of each class\n",
    "    class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "    # Create a balanced subset\n",
    "    balanced_indices = []\n",
    "    for class_label in class_counts:\n",
    "        class_indices = np.where(y_labels_chunk == class_label)[0]\n",
    "        balanced_indices.extend(class_indices)\n",
    "\n",
    "    # Select the balanced subset\n",
    "    X_rp_subset = X_rp_images_chunk[balanced_indices]\n",
    "    X_gaf_subset = X_gaf_images_chunk[balanced_indices]\n",
    "    X_mtf_subset = X_mtf_images_chunk[balanced_indices]\n",
    "    y_subset = y_labels_chunk[balanced_indices]\n",
    "\n",
    "    # Save the chunk to disk\n",
    "    np.save(os.path.join(save_dir, f'X_rp_chunk_{chunk_id}.npy'), X_rp_subset)\n",
    "    np.save(os.path.join(save_dir, f'X_gaf_chunk_{chunk_id}.npy'), X_gaf_subset)\n",
    "    np.save(os.path.join(save_dir, f'X_mtf_chunk_{chunk_id}.npy'), X_mtf_subset)\n",
    "    np.save(os.path.join(save_dir, f'y_chunk_{chunk_id}.npy'), y_subset)\n",
    "\n",
    "# Define directories\n",
    "save_dir = './split_datav5_full'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 5000  # Define a smaller chunk size to fit into memory\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        X_rp_images_chunk = hdf5_file['rp_images'][i:i+chunk_size]\n",
    "        X_gaf_images_chunk = hdf5_file['gaf_images'][i:i+chunk_size]\n",
    "        X_mtf_images_chunk = hdf5_file['mtf_images'][i:i+chunk_size]\n",
    "        y_labels_chunk = hdf5_file['labels'][i:i+chunk_size]\n",
    "\n",
    "        # Process and save each chunk\n",
    "        process_and_save_chunk(X_rp_images_chunk, X_gaf_images_chunk, X_mtf_images_chunk, y_labels_chunk, i // chunk_size, save_dir)\n",
    "\n",
    "# Concatenate the processed chunks from disk\n",
    "def load_and_concatenate_chunks(chunk_prefix, save_dir):\n",
    "    chunks = []\n",
    "    for chunk_file in sorted(os.listdir(save_dir)):\n",
    "        if chunk_file.startswith(chunk_prefix):\n",
    "            chunks.append(np.load(os.path.join(save_dir, chunk_file)))\n",
    "    return np.concatenate(chunks, axis=0)\n",
    "\n",
    "X_rp_images = load_and_concatenate_chunks('X_rp_chunk_', save_dir)\n",
    "X_gaf_images = load_and_concatenate_chunks('X_gaf_chunk_', save_dir)\n",
    "X_mtf_images = load_and_concatenate_chunks('X_mtf_chunk_', save_dir)\n",
    "y_labels = load_and_concatenate_chunks('y_chunk_', save_dir)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_images, X_gaf_images, X_mtf_images, y_labels, test_size=0.3, stratify=y_labels, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data\n",
    "np.save(os.path.join(save_dir, 'X_rp_train.npy'), X_rp_train)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_train.npy'), X_gaf_train)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_train.npy'), X_mtf_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_val.npy'), X_rp_val)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_val.npy'), X_gaf_val)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_val.npy'), X_mtf_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_test.npy'), X_rp_test)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_test.npy'), X_gaf_test)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_test.npy'), X_mtf_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split and saved in directory: ./split_datav5_full_compressed\n",
      "Number of samples per class in the training set:\n",
      "Class 1: 4906 samples\n",
      "Class 3: 63056 samples\n",
      "Class 0: 1947 samples\n",
      "Class 2: 561 samples\n",
      "Class 4: 10 samples\n",
      "\n",
      "Number of samples per class in the validation set:\n",
      "Class 3: 13512 samples\n",
      "Class 0: 417 samples\n",
      "Class 1: 1051 samples\n",
      "Class 2: 120 samples\n",
      "Class 4: 3 samples\n",
      "\n",
      "Number of samples per class in the test set:\n",
      "Class 3: 13512 samples\n",
      "Class 0: 417 samples\n",
      "Class 2: 121 samples\n",
      "Class 1: 1051 samples\n",
      "Class 4: 2 samples\n"
     ]
    }
   ],
   "source": [
    "#save data in a compressed format:\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to process each chunk and save it to disk\n",
    "def process_and_save_chunk(X_rp_images_chunk, X_gaf_images_chunk, X_mtf_images_chunk, y_labels_chunk, chunk_id, save_dir):\n",
    "    # Ensure we have a sufficient number of samples for each class\n",
    "    class_counts = Counter(y_labels_chunk)\n",
    "    total_counts = sum(class_counts.values())\n",
    "\n",
    "    # Calculate the proportions of each class\n",
    "    class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "    # Create a balanced subset\n",
    "    balanced_indices = []\n",
    "    for class_label in class_counts:\n",
    "        class_indices = np.where(y_labels_chunk == class_label)[0]\n",
    "        balanced_indices.extend(class_indices)\n",
    "\n",
    "    # Select the balanced subset\n",
    "    X_rp_subset = X_rp_images_chunk[balanced_indices]\n",
    "    X_gaf_subset = X_gaf_images_chunk[balanced_indices]\n",
    "    X_mtf_subset = X_mtf_images_chunk[balanced_indices]\n",
    "    y_subset = y_labels_chunk[balanced_indices]\n",
    "\n",
    "    # Save the chunk to disk in compressed format\n",
    "    np.savez_compressed(os.path.join(save_dir, f'chunk_{chunk_id}.npz'),\n",
    "                        X_rp=X_rp_subset, X_gaf=X_gaf_subset, X_mtf=X_mtf_subset, y=y_subset)\n",
    "\n",
    "# Define directories\n",
    "save_dir = './split_datav5_full_compressed'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 5000  # Define a smaller chunk size to fit into memory\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        X_rp_images_chunk = hdf5_file['rp_images'][i:i+chunk_size]\n",
    "        X_gaf_images_chunk = hdf5_file['gaf_images'][i:i+chunk_size]\n",
    "        X_mtf_images_chunk = hdf5_file['mtf_images'][i:i+chunk_size]\n",
    "        y_labels_chunk = hdf5_file['labels'][i:i+chunk_size]\n",
    "\n",
    "        # Process and save each chunk\n",
    "        process_and_save_chunk(X_rp_images_chunk, X_gaf_images_chunk, X_mtf_images_chunk, y_labels_chunk, i // chunk_size, save_dir)\n",
    "\n",
    "# Concatenate the processed chunks from disk\n",
    "def load_and_concatenate_chunks(save_dir):\n",
    "    X_rp_chunks = []\n",
    "    X_gaf_chunks = []\n",
    "    X_mtf_chunks = []\n",
    "    y_chunks = []\n",
    "\n",
    "    for chunk_file in sorted(os.listdir(save_dir)):\n",
    "        if chunk_file.endswith('.npz'):\n",
    "            data = np.load(os.path.join(save_dir, chunk_file))\n",
    "            X_rp_chunks.append(data['X_rp'])\n",
    "            X_gaf_chunks.append(data['X_gaf'])\n",
    "            X_mtf_chunks.append(data['X_mtf'])\n",
    "            y_chunks.append(data['y'])\n",
    "\n",
    "    X_rp_images = np.concatenate(X_rp_chunks, axis=0)\n",
    "    X_gaf_images = np.concatenate(X_gaf_chunks, axis=0)\n",
    "    X_mtf_images = np.concatenate(X_mtf_chunks, axis=0)\n",
    "    y_labels = np.concatenate(y_chunks, axis=0)\n",
    "\n",
    "    return X_rp_images, X_gaf_images, X_mtf_images, y_labels\n",
    "\n",
    "X_rp_images, X_gaf_images, X_mtf_images, y_labels = load_and_concatenate_chunks(save_dir)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_images, X_gaf_images, X_mtf_images, y_labels, test_size=0.3, stratify=y_labels, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data in compressed format\n",
    "np.savez_compressed(os.path.join(save_dir, 'train_data.npz'),\n",
    "                    X_rp=X_rp_train, X_gaf=X_gaf_train, X_mtf=X_mtf_train, y=y_train)\n",
    "np.savez_compressed(os.path.join(save_dir, 'val_data.npz'),\n",
    "                    X_rp=X_rp_val, X_gaf=X_gaf_val, X_mtf=X_mtf_val, y=y_val)\n",
    "np.savez_compressed(os.path.join(save_dir, 'test_data.npz'),\n",
    "                    X_rp=X_rp_test, X_gaf=X_gaf_test, X_mtf=X_mtf_test, y=y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtestv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
