{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 10000  # Define a chunk size that fits into memory\n",
    "max_chunks = 10  # Limit the number of chunks to process\n",
    "\n",
    "# Lists to store chunks\n",
    "X_segments_chunks = []\n",
    "y_labels_chunks = []\n",
    "X_rp_images_chunks = []\n",
    "X_gaf_images_chunks = []\n",
    "X_mtf_images_chunks = []\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        if len(X_segments_chunks) >= max_chunks:\n",
    "            break\n",
    "        X_segments_chunks.append(hdf5_file['segments'][i:i+chunk_size])\n",
    "        y_labels_chunks.append(hdf5_file['labels'][i:i+chunk_size])\n",
    "        X_rp_images_chunks.append(hdf5_file['rp_images'][i:i+chunk_size])\n",
    "        X_gaf_images_chunks.append(hdf5_file['gaf_images'][i:i+chunk_size])\n",
    "        X_mtf_images_chunks.append(hdf5_file['mtf_images'][i:i+chunk_size])\n",
    "\n",
    "# Concatenate the chunks into a single array\n",
    "X_segments = np.concatenate(X_segments_chunks, axis=0)\n",
    "y_labels = np.concatenate(y_labels_chunks, axis=0)\n",
    "X_rp_images = np.concatenate(X_rp_images_chunks, axis=0)\n",
    "X_gaf_images = np.concatenate(X_gaf_images_chunks, axis=0)\n",
    "X_mtf_images = np.concatenate(X_mtf_images_chunks, axis=0)\n",
    "\n",
    "# Exclude the VEB class (Label 4)\n",
    "exclude_class = 4\n",
    "included_indices = np.where(y_labels != exclude_class)[0]\n",
    "\n",
    "X_segments = X_segments[included_indices]\n",
    "y_labels = y_labels[included_indices]\n",
    "X_rp_images = X_rp_images[included_indices]\n",
    "X_gaf_images = X_gaf_images[included_indices]\n",
    "X_mtf_images = X_mtf_images[included_indices]\n",
    "\n",
    "# Ensure we have a sufficient number of samples for each class\n",
    "class_counts = Counter(y_labels)\n",
    "total_counts = sum(class_counts.values())\n",
    "\n",
    "# Calculate the proportions of each class\n",
    "class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "# Set the number of samples for the smallest class\n",
    "min_samples_per_class = 100  # Adjust this number if needed\n",
    "\n",
    "# Calculate the number of samples for each class based on the original ratios\n",
    "balanced_counts = {cls: int(min_samples_per_class / class_ratios[cls]) for cls in class_ratios}\n",
    "\n",
    "# Create a balanced subset\n",
    "balanced_indices = []\n",
    "for class_label, count in balanced_counts.items():\n",
    "    class_indices = np.where(y_labels == class_label)[0]\n",
    "    selected_indices = np.random.choice(class_indices, min(len(class_indices), count), replace=False)\n",
    "    balanced_indices.extend(selected_indices)\n",
    "\n",
    "# Select the balanced subset\n",
    "X_rp_subset = X_rp_images[balanced_indices]\n",
    "X_gaf_subset = X_gaf_images[balanced_indices]\n",
    "X_mtf_subset = X_mtf_images[balanced_indices]\n",
    "y_subset = y_labels[balanced_indices]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_subset, X_gaf_subset, X_mtf_subset, y_subset, test_size=0.3, stratify=y_subset, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data\n",
    "save_dir = './split_datav4'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, 'X_rp_train.npy'), X_rp_train)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_train.npy'), X_gaf_train)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_train.npy'), X_mtf_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_val.npy'), X_rp_val)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_val.npy'), X_gaf_val)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_val.npy'), X_mtf_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_test.npy'), X_rp_test)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_test.npy'), X_gaf_test)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_test.npy'), X_mtf_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split and saved in directory: ./split_datav4\n",
      "Number of samples per class in the training set:\n",
      "Class 1: 2996 samples\n",
      "Class 0: 1947 samples\n",
      "Class 3: 234 samples\n",
      "Class 2: 561 samples\n",
      "\n",
      "Number of samples per class in the validation set:\n",
      "Class 1: 642 samples\n",
      "Class 0: 417 samples\n",
      "Class 2: 120 samples\n",
      "Class 3: 51 samples\n",
      "\n",
      "Number of samples per class in the test set:\n",
      "Class 1: 642 samples\n",
      "Class 0: 417 samples\n",
      "Class 2: 121 samples\n",
      "Class 3: 50 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset in smaller chunks\n",
    "output_hdf5_path = 'final_datasetv3AllRecords.h5'\n",
    "chunk_size = 10000  # Define a chunk size that fits into memory\n",
    "max_chunks = 10  # Limit the number of chunks to process\n",
    "\n",
    "# Lists to store chunks\n",
    "X_segments_chunks = []\n",
    "y_labels_chunks = []\n",
    "X_rp_images_chunks = []\n",
    "X_gaf_images_chunks = []\n",
    "X_mtf_images_chunks = []\n",
    "\n",
    "# Open the HDF5 file and read the dataset in chunks\n",
    "with h5py.File(output_hdf5_path, 'r') as hdf5_file:\n",
    "    total_samples = hdf5_file['segments'].shape[0]\n",
    "\n",
    "    # Read data in chunks\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        if len(X_segments_chunks) >= max_chunks:\n",
    "            break\n",
    "        X_segments_chunks.append(hdf5_file['segments'][i:i+chunk_size])\n",
    "        y_labels_chunks.append(hdf5_file['labels'][i:i+chunk_size])\n",
    "        X_rp_images_chunks.append(hdf5_file['rp_images'][i:i+chunk_size])\n",
    "        X_gaf_images_chunks.append(hdf5_file['gaf_images'][i:i+chunk_size])\n",
    "        X_mtf_images_chunks.append(hdf5_file['mtf_images'][i:i+chunk_size])\n",
    "\n",
    "# Concatenate the chunks into a single array\n",
    "X_segments = np.concatenate(X_segments_chunks, axis=0)\n",
    "y_labels = np.concatenate(y_labels_chunks, axis=0)\n",
    "X_rp_images = np.concatenate(X_rp_images_chunks, axis=0)\n",
    "X_gaf_images = np.concatenate(X_gaf_images_chunks, axis=0)\n",
    "X_mtf_images = np.concatenate(X_mtf_images_chunks, axis=0)\n",
    "\n",
    "# Exclude the VEB class (Label 4)\n",
    "exclude_class = 4\n",
    "included_indices = np.where(y_labels != exclude_class)[0]\n",
    "\n",
    "X_segments = X_segments[included_indices]\n",
    "y_labels = y_labels[included_indices]\n",
    "X_rp_images = X_rp_images[included_indices]\n",
    "X_gaf_images = X_gaf_images[included_indices]\n",
    "X_mtf_images = X_mtf_images[included_indices]\n",
    "\n",
    "# Ensure we have a sufficient number of samples for each class\n",
    "class_counts = Counter(y_labels)\n",
    "total_counts = sum(class_counts.values())\n",
    "\n",
    "# Calculate the proportions of each class\n",
    "class_ratios = {cls: count / total_counts for cls, count in class_counts.items()}\n",
    "\n",
    "# Set the number of samples for the smallest class\n",
    "min_samples_per_class = 300  # Increase this number to ensure more samples per class\n",
    "\n",
    "# Calculate the number of samples for each class based on the original ratios\n",
    "balanced_counts = {cls: int(min_samples_per_class / class_ratios[cls]) for cls in class_ratios}\n",
    "\n",
    "# Create a balanced subset\n",
    "balanced_indices = []\n",
    "for class_label, count in balanced_counts.items():\n",
    "    class_indices = np.where(y_labels == class_label)[0]\n",
    "    selected_indices = np.random.choice(class_indices, min(len(class_indices), count), replace=False)\n",
    "    balanced_indices.extend(selected_indices)\n",
    "\n",
    "# Select the balanced subset\n",
    "X_rp_subset = X_rp_images[balanced_indices]\n",
    "X_gaf_subset = X_gaf_images[balanced_indices]\n",
    "X_mtf_subset = X_mtf_images[balanced_indices]\n",
    "y_subset = y_labels[balanced_indices]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_rp_train, X_rp_temp, X_gaf_train, X_gaf_temp, X_mtf_train, X_mtf_temp, y_train, y_temp = train_test_split(\n",
    "    X_rp_subset, X_gaf_subset, X_mtf_subset, y_subset, test_size=0.3, stratify=y_subset, random_state=42)\n",
    "X_rp_val, X_rp_test, X_gaf_val, X_gaf_test, X_mtf_val, X_mtf_test, y_val, y_test = train_test_split(\n",
    "    X_rp_temp, X_gaf_temp, X_mtf_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Save the split data\n",
    "save_dir = './split_datav4'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, 'X_rp_train.npy'), X_rp_train)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_train.npy'), X_gaf_train)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_train.npy'), X_mtf_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_val.npy'), X_rp_val)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_val.npy'), X_gaf_val)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_val.npy'), X_mtf_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'X_rp_test.npy'), X_rp_test)\n",
    "np.save(os.path.join(save_dir, 'X_gaf_test.npy'), X_gaf_test)\n",
    "np.save(os.path.join(save_dir, 'X_mtf_test.npy'), X_mtf_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"Data successfully split and saved in directory: {save_dir}\")\n",
    "\n",
    "# Check the number of samples per class in the training, validation, and test sets\n",
    "def count_samples_per_class(y):\n",
    "    class_counts = Counter(y)\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "print(\"Number of samples per class in the training set:\")\n",
    "count_samples_per_class(y_train)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the validation set:\")\n",
    "count_samples_per_class(y_val)\n",
    "\n",
    "print(\"\\nNumber of samples per class in the test set:\")\n",
    "count_samples_per_class(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtestv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
